{"name":"Machine learning","tagline":"by Daniel Godoy","body":"<!DOCTYPE html>\r\n<html>\r\n\r\n\r\n<body>\r\n<p><strong>This document contains the course project for the Machine Learning Course of the</strong>\r\n<strong>Johns Hopkins&#39; Data Science Specialization at Coursera.</strong></p>\r\n\r\n<h2>1. Background</h2>\r\n\r\n<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to\r\ncollect a large amount of data about personal activity relatively inexpensively.\r\nThese type of devices are part of the quantified self movement â€“ a group of enthusiasts\r\nwho take measurements about themselves regularly to improve their health, to find\r\npatterns in their behavior, or because they are tech geeks. One thing that people\r\nregularly do is quantify how much of a particular activity they do, but they rarely\r\nquantify how well they do it. In this project, the goal is to use data from\r\naccelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were\r\nasked to perform barbell lifts correctly and incorrectly in 5 different ways. More\r\ninformation is available from the website here: <a href=\"http://groupware.les.inf.puc-rio.br/har\">http://groupware.les.inf.puc-rio.br/har</a>\r\n(see the section on the Weight Lifting Exercise Dataset). </p>\r\n\r\n<h3>1.1. Data</h3>\r\n\r\n<p>The training data for this project are available here: </p>\r\n\r\n<p><a href=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>\r\n\r\n<p>The test data are available here: </p>\r\n\r\n<p><a href=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>\r\n\r\n<p>The data for this project come from this source: <a href=\"http://groupware.les.inf.puc-rio.br/har\">http://groupware.les.inf.puc-rio.br/har</a>.</p>\r\n\r\n<h2>2. Assignment</h2>\r\n\r\n<p>In order to complete the assignment, the followung steps were taken.</p>\r\n\r\n<ul>\r\n<li>Load the data</li>\r\n<li>Clean the data</li>\r\n<li>Prepare and Perform a Random Forest Model</li>\r\n<li>Predict resuls of the testing dataset.</li>\r\n</ul>\r\n\r\n<h3>2.1. Load the Data</h3>\r\n\r\n<p>The files were downloaded from the links provided before. The data was loaded into\r\ntwo different datasets: <em>trainingfile</em> and <em>testingfile</em>.</p>\r\n\r\n<pre><code class=\"r\">trainingfile &lt;- read.csv(trainingfileroute, header=TRUE,\r\n                     na.strings=c(&quot;NA&quot;, &quot;&quot;))\r\ntestingfile &lt;- read.csv(testingfileroute, header=TRUE,\r\n                     na.strings=c(&quot;NA&quot;, &quot;&quot;))\r\n</code></pre>\r\n\r\n<h3>2.2. Clean the Data</h3>\r\n\r\n<p>In order to prepare a good model, I removed data that may not contribute to the\r\nability to predict of the model. First, I removed variables that identified the\r\nsubject of study and the time the data was taken. Then, I removed variables\r\nthat contained more than 90% of NAs. Then, I removed data that does not vary\r\nsignificantly. </p>\r\n\r\n<p>Note that the data cleaning process was done to the trainingfile and testinfile\r\ndataset equally. Every process is explained below:</p>\r\n\r\n<h4>2.2.1. Removing Identification and Information Variables</h4>\r\n\r\n<p>After a quick inspection of the datasets, I noticed that the first six variables\r\ncorresponded to identification variables or information variables.  Since these\r\nvariables may not add prediction capacitiy to a model, I removed them.</p>\r\n\r\n<pre><code class=\"r\">trainingfile &lt;- trainingfile[, -(1:6)]\r\ntestingfile &lt;- testingfile[, -(1:6)]\r\n</code></pre>\r\n\r\n<h4>2.2.2. Removing Variables Composed Mainly by NAs</h4>\r\n\r\n<p>After a quick inspection of the datasets, I noticed that there were many variables\r\nthat were composed mainly by NAs (missing data). Since these variables may not\r\nadd prediction capacity to a model, I removed them.</p>\r\n\r\n<pre><code class=\"r\">minimumrows &lt;- nrow(trainingfile)*0.9\r\nnaspercolumn &lt;- sapply(trainingfile, function(x) sum(is.na(x)))\r\ntrainingfile &lt;- trainingfile[, naspercolumn &lt; minimumrows]\r\n\r\nminimumrows &lt;- nrow(testingfile)*0.9\r\nnaspercolumn &lt;- sapply(testingfile, function(x) sum(is.na(x)))\r\ntestingfile &lt;- testingfile[, naspercolumn &lt; minimumrows]\r\n</code></pre>\r\n\r\n<h4>2.2.3. Removing Low Varying Variables</h4>\r\n\r\n<p>After a quick inspection of the datasets, I noticed that there were vaiables that\r\ndid not vary much. Since these variables may not add prediction capacity to a model,\r\nI removed them. For doing this, I removed variables which its\r\n<a href=\"https://en.wikipedia.org/wiki/Coefficient_of_variation\">Coefficient of Variation</a>.\r\nwas less than 1. </p>\r\n\r\n<pre><code class=\"r\">variation &lt;- sapply(trainingfile, function(x) abs(sd(x)/mean(x)))\r\n</code></pre>\r\n\r\n<pre><code>## Warning in mean.default(x): argument is not numeric or logical: returning\r\n## NA\r\n</code></pre>\r\n\r\n<pre><code class=\"r\">trainingfile &lt;- trainingfile[ , -variation[-length(variation)] &lt; 1]\r\n\r\nvariation &lt;- sapply(testingfile, function(x) abs(sd(x)/mean(x)))\r\ntestingfile &lt;- testingfile[ , -variation[-length(variation)] &lt; 1]\r\n</code></pre>\r\n\r\n<h3>2.3. Prepare and Perform and Random Forest Model</h3>\r\n\r\n<p>To create a model that can predict the classe certain data belongs to, \r\nI used the Random Forest algorithm in the training dataset. The reasons I chose\r\nthe Random Forest algorithm are:</p>\r\n\r\n<ul>\r\n<li>The algorithm does not expect linear features on the variables</li>\r\n<li>The algorithm does not expect lienar relationships among variables</li>\r\n<li>The algorithm maages well large number of training samples</li>\r\n<li>The algorithm gives estimates of what variables are important in the classification</li>\r\n<li>The algorithm there does not need cross-validation as it is estimated internally,\r\nduring the execution.</li>\r\n</ul>\r\n\r\n<p>In order to prepare the data, I created a training and testing data set out of\r\nthe <em>trainingfile</em>.</p>\r\n\r\n<h4>2.3.1. Creating the Training and Testing Data Set</h4>\r\n\r\n<p>To create the Random Forest algorithm, the data on the trainingfile were split into\r\ntwo data sets: training and testing.</p>\r\n\r\n<pre><code class=\"r\">library(caret)\r\ninTrain &lt;- createDataPartition(trainingfile$classe, p=0.75, list=FALSE)\r\ntraining &lt;- trainingfile[inTrain, ]\r\ntesting &lt;- trainingfile[-inTrain, ]\r\n</code></pre>\r\n\r\n<h4>2.3.2. Creating the Random Forest Algorithm</h4>\r\n\r\n<p>The random forest algorithm was crated using the randomForest package in R.</p>\r\n\r\n<pre><code class=\"r\">library(randomForest)\r\nset.seed(1234)\r\nmodelFit &lt;- randomForest(classe~., data=training, importance=TRUE)\r\n</code></pre>\r\n\r\n<p>The results of the algorithm were:</p>\r\n\r\n<pre><code class=\"r\">modelFit\r\n</code></pre>\r\n\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(formula = classe ~ ., data = training, importance = TRUE) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 7\r\n## \r\n##         OOB estimate of  error rate: 0.22%\r\n## Confusion matrix:\r\n##      A    B    C    D    E  class.error\r\n## A 4184    0    0    0    1 0.0002389486\r\n## B    4 2842    2    0    0 0.0021067416\r\n## C    0    8 2559    0    0 0.0031164784\r\n## D    0    0   10 2401    1 0.0045605307\r\n## E    0    0    0    7 2699 0.0025868441\r\n</code></pre>\r\n\r\n<p>As the results show, every time we only randomly used 7 predictorts. Additionally,\r\nThe error rate is really small.</p>\r\n\r\n<h4>2.3.3. Evaluation of the Altogrithm</h4>\r\n\r\n<p>To evaluate the algorithm, we will predict the outcome(classe) of the testing dataset\r\ncreated in section 2.3.1 and evaluate the results using a Confusion Matrix.</p>\r\n\r\n<pre><code class=\"r\">confusionMatrix(predict(modelFit, newdata=testing[, -ncol(testing)]),\r\n                testing$classe)\r\n</code></pre>\r\n\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1395    0    0    0    0\r\n##          B    0  949    2    0    0\r\n##          C    0    0  853    4    0\r\n##          D    0    0    0  800    0\r\n##          E    0    0    0    0  901\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9988          \r\n##                  95% CI : (0.9973, 0.9996)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9985          \r\n##  Mcnemar&#39;s Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   1.0000   0.9977   0.9950   1.0000\r\n## Specificity            1.0000   0.9995   0.9990   1.0000   1.0000\r\n## Pos Pred Value         1.0000   0.9979   0.9953   1.0000   1.0000\r\n## Neg Pred Value         1.0000   1.0000   0.9995   0.9990   1.0000\r\n## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Detection Rate         0.2845   0.1935   0.1739   0.1631   0.1837\r\n## Detection Prevalence   0.2845   0.1939   0.1748   0.1631   0.1837\r\n## Balanced Accuracy      1.0000   0.9997   0.9983   0.9975   1.0000\r\n</code></pre>\r\n\r\n<p>As the table above shows, the accuracy of the prediction model s pretty good. \r\nAdditionally, the kappa measurement is also very good.</p>\r\n\r\n<h3>3. Predicting the Classe of the Testing File</h3>\r\n\r\n<p>I predicted the class of the observations that were on the testing file using\r\nthe algorithm built in section 2.3.2. </p>\r\n\r\n<pre><code class=\"r\">predictions &lt;- predict(modelFit,newdata=testingfile)\r\npredictions\r\n</code></pre>\r\n\r\n<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n## Levels: A B C D E\r\n</code></pre>\r\n\r\n<p>The data was used in the Submission of the course project. </p>\r\n\r\n<hr>\r\n\r\n<p>END</p>\r\n\r\n</body>\r\n\r\n</html>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}