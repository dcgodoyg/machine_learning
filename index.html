<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Machine learning by dcgodoyg</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Machine learning</h1>
        <p>by Daniel Godoy</p>

        <p class="view"><a href="https://github.com/dcgodoyg/machine_learning">View the Project on GitHub <small>dcgodoyg/machine_learning</small></a></p>


        <ul>
          <li><a href="https://github.com/dcgodoyg/machine_learning/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/dcgodoyg/machine_learning/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/dcgodoyg/machine_learning">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <p>&lt;!DOCTYPE html&gt;
</p>

<p></p>

<p><strong>This document contains the course project for the Machine Learning Course of the</strong>
<strong>Johns Hopkins' Data Science Specialization at Coursera.</strong></p>

<h2>
<a id="1-background" class="anchor" href="#1-background" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Background</h2>

<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to
collect a large amount of data about personal activity relatively inexpensively.
These type of devices are part of the quantified self movement â€“ a group of enthusiasts
who take measurements about themselves regularly to improve their health, to find
patterns in their behavior, or because they are tech geeks. One thing that people
regularly do is quantify how much of a particular activity they do, but they rarely
quantify how well they do it. In this project, the goal is to use data from
accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were
asked to perform barbell lifts correctly and incorrectly in 5 different ways. More
information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>
(see the section on the Weight Lifting Exercise Dataset). </p>

<h3>
<a id="11-data" class="anchor" href="#11-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1. Data</h3>

<p>The training data for this project are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<p>The test data are available here: </p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<p>The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>.</p>

<h2>
<a id="2-assignment" class="anchor" href="#2-assignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Assignment</h2>

<p>In order to complete the assignment, the followung steps were taken.</p>

<ul>
<li>Load the data</li>
<li>Clean the data</li>
<li>Prepare and Perform a Random Forest Model</li>
<li>Predict resuls of the testing dataset.</li>
</ul>

<h3>
<a id="21-load-the-data" class="anchor" href="#21-load-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. Load the Data</h3>

<p>The files were downloaded from the links provided before. The data was loaded into
two different datasets: <em>trainingfile</em> and <em>testingfile</em>.</p>

<pre><code>trainingfile &lt;- read.csv(trainingfileroute, header=TRUE,
                     na.strings=c("NA", ""))
testingfile &lt;- read.csv(testingfileroute, header=TRUE,
                     na.strings=c("NA", ""))
</code></pre>

<h3>
<a id="22-clean-the-data" class="anchor" href="#22-clean-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. Clean the Data</h3>

<p>In order to prepare a good model, I removed data that may not contribute to the
ability to predict of the model. First, I removed variables that identified the
subject of study and the time the data was taken. Then, I removed variables
that contained more than 90% of NAs. Then, I removed data that does not vary
significantly. </p>

<p>Note that the data cleaning process was done to the trainingfile and testinfile
dataset equally. Every process is explained below:</p>

<h4>
<a id="221-removing-identification-and-information-variables" class="anchor" href="#221-removing-identification-and-information-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.1. Removing Identification and Information Variables</h4>

<p>After a quick inspection of the datasets, I noticed that the first six variables
corresponded to identification variables or information variables.  Since these
variables may not add prediction capacitiy to a model, I removed them.</p>

<pre><code>trainingfile &lt;- trainingfile[, -(1:6)]
testingfile &lt;- testingfile[, -(1:6)]
</code></pre>

<h4>
<a id="222-removing-variables-composed-mainly-by-nas" class="anchor" href="#222-removing-variables-composed-mainly-by-nas" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.2. Removing Variables Composed Mainly by NAs</h4>

<p>After a quick inspection of the datasets, I noticed that there were many variables
that were composed mainly by NAs (missing data). Since these variables may not
add prediction capacity to a model, I removed them.</p>

<pre><code>minimumrows &lt;- nrow(trainingfile)*0.9
naspercolumn &lt;- sapply(trainingfile, function(x) sum(is.na(x)))
trainingfile &lt;- trainingfile[, naspercolumn &lt; minimumrows]

minimumrows &lt;- nrow(testingfile)*0.9
naspercolumn &lt;- sapply(testingfile, function(x) sum(is.na(x)))
testingfile &lt;- testingfile[, naspercolumn &lt; minimumrows]
</code></pre>

<h4>
<a id="223-removing-low-varying-variables" class="anchor" href="#223-removing-low-varying-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.3. Removing Low Varying Variables</h4>

<p>After a quick inspection of the datasets, I noticed that there were vaiables that
did not vary much. Since these variables may not add prediction capacity to a model,
I removed them. For doing this, I removed variables which its
<a href="https://en.wikipedia.org/wiki/Coefficient_of_variation">Coefficient of Variation</a>.
was less than 1. </p>

<pre><code>variation &lt;- sapply(trainingfile, function(x) abs(sd(x)/mean(x)))
</code></pre>

<pre><code>## Warning in mean.default(x): argument is not numeric or logical: returning
## NA
</code></pre>

<pre><code>trainingfile &lt;- trainingfile[ , -variation[-length(variation)] &lt; 1]

variation &lt;- sapply(testingfile, function(x) abs(sd(x)/mean(x)))
testingfile &lt;- testingfile[ , -variation[-length(variation)] &lt; 1]
</code></pre>

<h3>
<a id="23-prepare-and-perform-and-random-forest-model" class="anchor" href="#23-prepare-and-perform-and-random-forest-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3. Prepare and Perform and Random Forest Model</h3>

<p>To create a model that can predict the classe certain data belongs to, 
I used the Random Forest algorithm in the training dataset. The reasons I chose
the Random Forest algorithm are:</p>

<ul>
<li>The algorithm does not expect linear features on the variables</li>
<li>The algorithm does not expect lienar relationships among variables</li>
<li>The algorithm maages well large number of training samples</li>
<li>The algorithm gives estimates of what variables are important in the classification</li>
<li>The algorithm there does not need cross-validation as it is estimated internally,
during the execution.</li>
</ul>

<p>In order to prepare the data, I created a training and testing data set out of
the <em>trainingfile</em>.</p>

<h4>
<a id="231-creating-the-training-and-testing-data-set" class="anchor" href="#231-creating-the-training-and-testing-data-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.1. Creating the Training and Testing Data Set</h4>

<p>To create the Random Forest algorithm, the data on the trainingfile were split into
two data sets: training and testing.</p>

<pre><code>library(caret)
inTrain &lt;- createDataPartition(trainingfile$classe, p=0.75, list=FALSE)
training &lt;- trainingfile[inTrain, ]
testing &lt;- trainingfile[-inTrain, ]
</code></pre>

<h4>
<a id="232-creating-the-random-forest-algorithm" class="anchor" href="#232-creating-the-random-forest-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.2. Creating the Random Forest Algorithm</h4>

<p>The random forest algorithm was crated using the randomForest package in R.</p>

<pre><code>library(randomForest)
set.seed(1234)
modelFit &lt;- randomForest(classe~., data=training, importance=TRUE)
</code></pre>

<p>The results of the algorithm were:</p>

<pre><code>modelFit
</code></pre>

<pre><code>## 
## Call:
##  randomForest(formula = classe ~ ., data = training, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 7
## 
##         OOB estimate of  error rate: 0.22%
## Confusion matrix:
##      A    B    C    D    E  class.error
## A 4184    0    0    0    1 0.0002389486
## B    4 2842    2    0    0 0.0021067416
## C    0    8 2559    0    0 0.0031164784
## D    0    0   10 2401    1 0.0045605307
## E    0    0    0    7 2699 0.0025868441
</code></pre>

<p>As the results show, every time we only randomly used 7 predictorts. Additionally,
The error rate is really small.</p>

<h4>
<a id="233-evaluation-of-the-altogrithm" class="anchor" href="#233-evaluation-of-the-altogrithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.3. Evaluation of the Altogrithm</h4>

<p>To evaluate the algorithm, we will predict the outcome(classe) of the testing dataset
created in section 2.3.1 and evaluate the results using a Confusion Matrix.</p>

<pre><code>confusionMatrix(predict(modelFit, newdata=testing[, -ncol(testing)]),
                testing$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    0  949    2    0    0
##          C    0    0  853    4    0
##          D    0    0    0  800    0
##          E    0    0    0    0  901
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9988          
##                  95% CI : (0.9973, 0.9996)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9985          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   1.0000   0.9977   0.9950   1.0000
## Specificity            1.0000   0.9995   0.9990   1.0000   1.0000
## Pos Pred Value         1.0000   0.9979   0.9953   1.0000   1.0000
## Neg Pred Value         1.0000   1.0000   0.9995   0.9990   1.0000
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2845   0.1935   0.1739   0.1631   0.1837
## Detection Prevalence   0.2845   0.1939   0.1748   0.1631   0.1837
## Balanced Accuracy      1.0000   0.9997   0.9983   0.9975   1.0000
</code></pre>

<p>As the table above shows, the accuracy of the prediction model s pretty good. 
Additionally, the kappa measurement is also very good.</p>

<h3>
<a id="3-predicting-the-classe-of-the-testing-file" class="anchor" href="#3-predicting-the-classe-of-the-testing-file" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Predicting the Classe of the Testing File</h3>

<p>I predicted the class of the observations that were on the testing file using
the algorithm built in section 2.3.2. </p>

<pre><code>predictions &lt;- predict(modelFit,newdata=testingfile)
predictions
</code></pre>

<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
</code></pre>

<p>The data was used in the Submission of the course project. </p>

<hr>

<p>END</p>

<p></p>

<p></p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/dcgodoyg">dcgodoyg</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
